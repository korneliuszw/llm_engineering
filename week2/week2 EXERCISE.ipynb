{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import ollama\n",
    "from anthropic import Anthropic \n",
    "from IPython.display import display, Markdown, update_display\n",
    "import gradio as gr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "65a51b25-4a8c-4c91-9e7f-2528d552f4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL_GPT = 'gpt-5-mini-2025-08-07'\n",
    "MODEL_GPT=\"o4-mini-2025-04-16\"\n",
    "MODEL_CLAUDE = \"claude-sonnet-4-20250514\"\n",
    "MODEL_OLLAMA = 'qwen3:8b'\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d201b4c7-88c6-4501-8352-aaf860023d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a tech wizard. Take questions from your apprentice and provide deep but easy to understand explanations for questions in markdown format suitable for Gradio that runs in browser. Be a good teacher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a7656a99-3016-410f-9644-9581a70cada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "claude = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5e28aa3d-1807-4e90-9a31-4b21e7822e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def mermaid_to_svg(uml_definition: str, output_path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Convert a Mermaid UML definition to an SVG file.\n",
    "\n",
    "    Args:\n",
    "        uml_definition (str): Mermaid UML definition.\n",
    "        output_path (str): Path where SVG will be saved. If None, a temp file is used.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the generated SVG file.\n",
    "    \"\"\"\n",
    "    # Ensure mermaid-cli (mmdc) is installed\n",
    "    if not shutil.which(\"mmdc\"):\n",
    "        raise RuntimeError(\"Mermaid CLI (mmdc) not found. Install with: npm install -g @mermaid-js/mermaid-cli\")\n",
    "\n",
    "    # Create a temporary .mmd file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mmd\", mode=\"w\") as tmp_file:\n",
    "        tmp_file.write(uml_definition)\n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    # If no output path provided, generate one\n",
    "    if output_path is None:\n",
    "        output_path = tmp_file_path.replace(\".mmd\", \".svg\")\n",
    "\n",
    "    # Run Mermaid CLI\n",
    "    subprocess.run(\n",
    "        [\"powershell\", \"C:/Users/kwojn/AppData/Roaming/npm/mmdc.ps1\", \"-i\", tmp_file_path, \"-o\", output_path],\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "    # Clean up .mmd file\n",
    "    os.remove(tmp_file_path)\n",
    "\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "97060403-dc84-451f-b0c9-7ec341c5ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_tools():\n",
    "    return [{\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"generate_diagram\",\n",
    "        \"description\": \"Generates image of a diagram from Mermaid input\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"definition\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Mermaid diagram definition\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"definition\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }]\n",
    "\n",
    "def generate_diagram(definition):\n",
    "    return mermaid_to_svg(definition)\n",
    "\n",
    "def get_call_function(tool):\n",
    "    match tool.name:\n",
    "        case \"generate_diagram\":\n",
    "            return generate_diagram\n",
    "def handle_tools(tool_calls):\n",
    "    output = []\n",
    "    for tool in tool_calls:\n",
    "        print(tool)\n",
    "        args = json.loads(tool.arguments)\n",
    "        func = get_call_function(tool)\n",
    "        result = func(**args)\n",
    "        output.append({\n",
    "            \"type\": \"function_call_output\",\n",
    "            \"call_id\": tool.call_id,\n",
    "            \"output\": json.dumps(result)\n",
    "        })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d0899b5d-b615-4e71-a1eb-b0357165d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_ollama(system_prompt, prompt, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_OLLAMA,\n",
    "        messages=messages,\n",
    "        tools=get_tools(),\n",
    "        think=True\n",
    "    )\n",
    "    print(response)\n",
    "    if \"tool_calls\" in response[\"message\"]:\n",
    "        handle_tools(response[\"message\"][\"tool_calls\"])\n",
    "    if \"thinking\" in response.message:\n",
    "        yield gr.ChatMessage(\n",
    "            role=\"assistant\",\n",
    "            content=response.message.thinking,\n",
    "            metadata={\"title\": \"Thought a bit\", \n",
    "                      \"status\": \"done\",\n",
    "                      \"duration\": response.total_duration / 1e9\n",
    "                     },\n",
    "        )\n",
    "    yield gr.ChatMessage(role=\"assistant\", content=response.message.content)\n",
    "\n",
    "previous_openai_id = None\n",
    "def ask_openai(system_prompt, prompt, history):\n",
    "    global previous_openai_id\n",
    "    inputs = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.responses.create(\n",
    "        model=MODEL_GPT,\n",
    "        input=inputs,\n",
    "        instructions=system_prompt,\n",
    "        previous_response_id=previous_openai_id,\n",
    "        tools=get_tools(),\n",
    "        max_output_tokens=2000\n",
    "    )\n",
    "    previous_openai_id = response.id\n",
    "    print(response)\n",
    "    tool_calls = list(filter(lambda item: item.type == \"function_call\", response.output))\n",
    "    if len(tool_calls) > 0:\n",
    "        tool_responses = handle_tools(tool_calls)\n",
    "        for tool in tool_responses:\n",
    "            yield gr.ChatMessage(\n",
    "                role=\"assistant\",\n",
    "                content=tool[\"output\"],\n",
    "                metadata={\"title\": \"Generated diagram\", \n",
    "                          \"status\": \"done\",\n",
    "                         },\n",
    "            )\n",
    "        response = openai.responses.create(\n",
    "            model=MODEL_GPT,\n",
    "            input=inputs + tool_responses,\n",
    "            instructions=system_prompt,\n",
    "            previous_response_id=previous_openai_id,\n",
    "            tools=get_tools(),\n",
    "            max_output_tokens=2000\n",
    "        )\n",
    "    for message in response.output:\n",
    "        print(message)\n",
    "        if message.type == \"reasoning\" and message.status:\n",
    "            yield gr.ChatMessage(\n",
    "                role=\"assistant\",\n",
    "                content='\\n'.join([thinking.text for thinking in message.content]),\n",
    "                metadata={\"title\": \"Thought a bit\", \n",
    "                          \"status\": \"done\",\n",
    "                         },\n",
    "            )\n",
    "        elif message.type == \"message\":\n",
    "            for content in message.content:\n",
    "                yield gr.ChatMessage(role=\"assistant\", content=content.text)\n",
    "    \n",
    "\n",
    "def select_ai(provider):\n",
    "    if provider == \"OpenAI\":\n",
    "        return ask_openai\n",
    "    elif provider == \"Local\":\n",
    "        return ask_ollama\n",
    "    raise \"AI provider not supported\"\n",
    "\n",
    "def chat(prompt, history, provider):\n",
    "    func = select_ai(provider)\n",
    "    history = [{\"content\": hist[\"content\"], \"role\": hist[\"role\"]} for hist in history]\n",
    "    responses = func(system_message, prompt, history)\n",
    "    for response in responses:\n",
    "        history.append(response)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0b3beeca-7ceb-444e-b094-43e879d47373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(audio_file):\n",
    "    print(audio_file)\n",
    "    if audio_file is None:\n",
    "        return \"\"\n",
    "    \n",
    "    with open(audio_file, \"rb\") as f:\n",
    "        transcript = openai.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=f\n",
    "        )\n",
    "    print(transcript)\n",
    "    return transcript.text\n",
    "\n",
    "def handle_transcribe_btn(audio, history, model):\n",
    "    text = transcribe(audio)\n",
    "    if text.strip() == \"\":\n",
    "        return history\n",
    "    response = chat(text, history, model)\n",
    "    print(response)\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c3459f13-7245-4f8b-a34a-ca65f3036638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7925\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7925/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwojn\\AppData\\Local\\Temp\\gradio\\f4727ea0a9ce35cbabd9b14ebc36d20949c315d8facd7a39b0a3ecd52182c94b\\audio.wav\n",
      "Transcription(text='Halo, raz, dwa, trzy, cztery, pięć.', logprobs=None, usage=UsageDuration(duration=None, type='duration', seconds=2))\n",
      "model='qwen3:8b' created_at='2025-08-19T18:59:41.1873424Z' done=True done_reason='stop' total_duration=20508081300 load_duration=5817695200 prompt_eval_count=175 prompt_eval_duration=687373700 eval_count=275 eval_duration=13993477500 message=Message(role='assistant', content='Hello! It looks like you\\'re saying \"Hello\" in Polish (\"Halo\") and listing numbers from one to five in Polish:  \\n1. **raz** (one)  \\n2. **dwa** (two)  \\n3. **trzy** (three)  \\n4. **cztery** (four)  \\n5. **pięć** (five)  \\n\\nAre you learning Polish, or is there something specific you\\'d like to explore? 😊', thinking='Okay, the user sent \"Halo, raz, dwa, trzy, cztery, pięć.\" Let me break this down. \"Halo\" is Polish for \"Hello,\" so they might be greeting me. Then there\\'s a list of words: \"raz,\" \"dwa,\" \"trzy,\" \"cztery,\" \"pięć.\" These are Polish words for one, two, three, four, five. So they\\'re listing numbers in Polish.\\n\\nI need to respond in a way that\\'s helpful and educational. Since they mentioned numbers, maybe they\\'re learning Polish or just testing if I recognize the language. My role is to explain things clearly, so I should acknowledge their message, point out the numbers, and maybe offer help with something related. Let me make sure to keep it friendly and open-ended in case they have more questions.\\n', images=None, tool_calls=None)\n",
      "[ChatMessage(content='Okay, the user sent \"Halo, raz, dwa, trzy, cztery, pięć.\" Let me break this down. \"Halo\" is Polish for \"Hello,\" so they might be greeting me. Then there\\'s a list of words: \"raz,\" \"dwa,\" \"trzy,\" \"cztery,\" \"pięć.\" These are Polish words for one, two, three, four, five. So they\\'re listing numbers in Polish.\\n\\nI need to respond in a way that\\'s helpful and educational. Since they mentioned numbers, maybe they\\'re learning Polish or just testing if I recognize the language. My role is to explain things clearly, so I should acknowledge their message, point out the numbers, and maybe offer help with something related. Let me make sure to keep it friendly and open-ended in case they have more questions.\\n', role='assistant', metadata={'title': 'Thought a bit', 'status': 'done', 'duration': 20.5080813}, options=[]), ChatMessage(content='Hello! It looks like you\\'re saying \"Hello\" in Polish (\"Halo\") and listing numbers from one to five in Polish:  \\n1. **raz** (one)  \\n2. **dwa** (two)  \\n3. **trzy** (three)  \\n4. **cztery** (four)  \\n5. **pięć** (five)  \\n\\nAre you learning Polish, or is there something specific you\\'d like to explore? 😊', role='assistant', metadata={}, options=[])]\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    provider = gr.Dropdown([\"OpenAI\", \"Local\"], label=\"Select AI\", value=\"Local\")\n",
    "    with gr.Row():\n",
    "        voice=gr.Microphone(type=\"filepath\")\n",
    "        transcribe_btn = gr.Button(\"Transcribe\")\n",
    "    chatter = gr.ChatInterface(fn=chat, type=\"messages\", additional_inputs=[provider])\n",
    "    transcribe_btn.click(\n",
    "        handle_transcribe_btn,\n",
    "        [voice, chatter.chatbot, provider],\n",
    "        chatter.chatbot\n",
    "    )\n",
    "ui.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
